# E-QUEST Framework: Benchmarking Methodology

This document details the scientific methodology employed by the E-QUEST framework to ensure a fair, robust, and insightful comparison between classical and quantum machine learning algorithms.

## 1. The Benchmarking Task

The framework focuses on a canonical problem in High-Energy Physics: **particle track segment classification**.

*   **Input Data:** The raw data is sourced from the TrackML Particle Tracking Challenge dataset. It consists of 3D "hits" generated by simulated particle collisions in a silicon detector.
*   **Data Preprocessing:** A "cone search" heuristic is used to generate candidate track segments, each composed of two hits on adjacent detector layers. This is a computationally efficient method to create a labeled dataset from the raw simulation output.
*   **The ML Problem:** The task is a binary classification problem: given the geometric features of a two-hit segment (`delta_r`, `delta_phi`, `delta_z`), predict whether the segment is "true" (both hits belong to the same particle) or "false".

## 2. The Algorithms Under Test

### Classical Baseline: Multi-Layer Perceptron (MLP)

*   **Description:** A standard, fully-connected neural network implemented in PyTorch was chosen as the classical baseline. Its architecture is simple but representative of a common deep learning approach to classification tasks.
*   **Rationale:** The MLP is a well-understood, universal function approximator, making it an excellent benchmark for the general-purpose capabilities of classical deep learning on this task.

### Quantum Challenger: Variational Quantum Classifier (VQC)

*   **Description:** A hybrid quantum-classical algorithm implemented in PennyLane. The model consists of a quantum circuit that acts as a trainable feature map, with its parameters optimized by a classical PyTorch optimizer.
*   **Circuit Architecture:**
    *   **Embedding:** `qml.templates.AngleEmbedding` is used to encode the 3 classical features into the initial state of the 4-qubit system.
    *   **Variational Layers (Ansatz):** `qml.templates.StronglyEntanglingLayers` are used as the trainable part of the circuit. This is a powerful and expressive ansatz known for its good performance on classification tasks.
*   **Rationale:** The VQC is a leading candidate for near-term quantum machine learning. This implementation allows for a direct comparison of its resource requirements against the classical MLP.

## 3. Key Metrics & Measurement Techniques

The framework is designed to capture a multi-dimensional view of performance and sustainability.

### Empirical (Hardware-Dependent) Metrics

These metrics are measured directly during the execution of the benchmarks.

*   **Computation Time:** To ensure scientific validity, the framework uses a **hardware-aware timer**.
    *   On systems with an NVIDIA GPU, it uses the high-precision `torch.cuda.Event` timer to measure only the time spent on GPU computation, excluding CPU overhead.
    *   On CPU-only systems, it gracefully falls back to the high-precision `time.perf_counter()` to measure CPU computation time.
*   **Peak Memory Usage:** Measured using `torch.cuda.max_memory_allocated()`, this captures the peak memory footprint of the algorithm (model, data batches, and framework overhead) on the GPU.
*   **Performance Metrics:**
    *   **ROC AUC Score:** The primary metric for classification performance, as it is robust to class imbalance.
    *   **Precision & Recall:** These metrics provide deeper insight into the model's behavior on the highly imbalanced track segment dataset.

### Projected (Hardware-Independent) Metrics

These metrics are calculated based on the fundamental properties of the algorithms, providing a forecast of their future potential.

*   **Projected Classical Energy:** Calculated as `(Total MAC Operations) * (Energy per MAC)`. This model assumes a constant energy cost for the fundamental "multiply-accumulate" operation of a classical processor.
*   **Projected Quantum Energy:** Calculated as `(Total Gate Operations) * (Energy per Gate)`. This model assumes a constant energy cost for the fundamental single-qubit and two-qubit gate operations of a future quantum computer. It is derived from two key metrics:
    *   **Gate Count:** The total number of fundamental quantum gates (`RX`, `Rot`, `CNOT`, etc.) after the high-level circuit is decomposed. This is a measure of the total computational work.
    *   **Circuit Depth:** The longest path of sequential gates in the decomposed circuit. This is a proxy for the algorithm's parallel runtimeâ€”a lower depth implies a faster potential execution time on real quantum hardware.

### Normalization for Trend Comparison

To facilitate a direct comparison of algorithmic scaling behavior, the framework also generates normalized plots. By dividing each data series by its first value, we can plot the "Scaling Factor" for each metric. This removes the misleading absolute units and allows for a direct visual comparison of the *trends*, which is the most robust scientific insight.